{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/competitions/reconocimiento-de-expresiones-faciales"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","from skimage.io import imread\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_data(directorio):\n","    pixel = [] \n","    expression = []\n","    for folder in os.listdir(directorio):\n","        print(folder)\n","        if os.path.isdir('/'.join([directorio, folder])):\n","            for file in os.listdir('/'.join([directorio, folder])):\n","\n","                image = imread('/'.join([directorio, folder, file]))\n","\n","                pixel.append(image)\n","                expression.append(folder)\n","\n","    return np.array(pixel),np.array(expression)\n","\n","def read_data_test(directorio):\n","    pixel = [] \n","    for folder in os.listdir(directorio):\n","        #print(folder)\n","        #if os.path.isdir('/'.join([directorio, folder])):\n","        #for file in os.listdir('/'.join([directorio, folder])):\n","\n","        image = imread('/'.join([directorio, folder]))\n","\n","        pixel.append(image)\n","\n","\n","    return np.array(pixel)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["angry\n","disgust\n","fear\n","happy\n","neutral\n","sad\n","surprise\n"]}],"source":["X_train_array, y_train_array = read_data('./data/train')\n","\n","true_test = read_data_test('./data/test')\n","\n","# Barajo las dos arrays en el mismo orden:\n","\n","indices = np.random.permutation(X_train_array.shape[0])\n","X_train_shuffled = X_train_array[indices]\n","y_train_shuffled = y_train_array[indices]\n","\n","# ---\n","\n","X_test = X_train_shuffled[:4611]\n","X_val = X_train_shuffled[4612:9222]\n","X_train = X_train_shuffled[9223:]\n","y_test = y_train_shuffled[:4611]\n","y_val = y_train_shuffled[4612:9222]\n","y_train = y_train_shuffled[9223:]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["################# MODIFICACIONES ###########################"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["X_train = X_train / 255\n","X_val = X_val / 255\n","X_test = X_test / 255"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_rgb_images = np.stack((X_train,) * 3, axis=-1)\n","val_rgb_images = np.stack((X_val,) * 3, axis=-1)\n","test_rgb_images = np.stack((X_test,) * 3, axis=-1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["##########################################################"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["dict_emotions = {\n","    'angry':0,\n","    'disgust':1,\n","    'fear':2,\n","    'happy':3,\n","    'neutral':4,\n","    'sad':5,\n","    'surprise':6\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["y_train_mapped = np.array([dict_emotions[emotion] for emotion in y_train])\n","y_val_mapped = np.array([dict_emotions[emotion] for emotion in y_val])\n","y_test_mapped = np.array([dict_emotions[emotion] for emotion in y_test])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","train_labels_one_hot = to_categorical(y_train_mapped, num_classes=7)\n","val_labels_one_hot = to_categorical(y_val_mapped, num_classes=7)\n","test_labels_one_hot = to_categorical(y_test_mapped, num_classes=7)"]},{"cell_type":"markdown","metadata":{},"source":["# MODELO"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\annav\\AppData\\Local\\Temp\\ipykernel_7604\\989208286.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","  base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n"]}],"source":["base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["for layer in base_model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\annav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))  # Asegúrate de que el input_shape tenga 3 canales\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(7, activation='softmax'))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["model_base_output = base_model.output\n","x = layers.Flatten()(model_base_output)\n","x = layers.Dense(512, activation='relu')(x)\n","output = layers.Dense(7, activation='softmax')(x)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["model = tf.keras.models.Model(inputs=base_model.input, outputs=output)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["(19598,)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["y_train_mapped.shape"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(19598, 48, 48, 3)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_rgb_images.shape"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 50ms/step - accuracy: 0.2716 - loss: 1.7701 - val_accuracy: 0.2928 - val_loss: 1.7469\n","Epoch 2/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.2950 - loss: 1.7255 - val_accuracy: 0.2911 - val_loss: 1.7404\n","Epoch 3/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.3019 - loss: 1.7185 - val_accuracy: 0.2980 - val_loss: 1.7361\n","Epoch 4/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.3105 - loss: 1.7067 - val_accuracy: 0.2948 - val_loss: 1.7317\n","Epoch 5/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - accuracy: 0.3109 - loss: 1.7049 - val_accuracy: 0.2805 - val_loss: 1.7453\n","Epoch 6/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 50ms/step - accuracy: 0.3102 - loss: 1.6986 - val_accuracy: 0.2837 - val_loss: 1.7338\n","Epoch 7/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.3131 - loss: 1.6940 - val_accuracy: 0.2939 - val_loss: 1.7411\n","Epoch 8/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.3187 - loss: 1.6843 - val_accuracy: 0.2924 - val_loss: 1.7311\n","Epoch 9/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 43ms/step - accuracy: 0.3216 - loss: 1.6755 - val_accuracy: 0.2954 - val_loss: 1.7318\n","Epoch 10/10\n","\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.3235 - loss: 1.6754 - val_accuracy: 0.2941 - val_loss: 1.7356\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x20559bfaa80>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(train_rgb_images, train_labels_one_hot, epochs=10, batch_size=32, validation_data=(val_rgb_images, val_labels_one_hot))"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.3028 - loss: 1.7283\n"]},{"data":{"text/plain":["[1.7338836193084717, 0.29819995164871216]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate(test_rgb_images,test_labels_one_hot)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step\n"]}],"source":["predictions = model.predict(test_rgb_images)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["predicted_classes = np.argmax(predictions, axis=1)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       angry       0.22      0.08      0.11       617\n","     disgust       1.00      0.01      0.03        77\n","        fear       0.22      0.09      0.13       690\n","       happy       0.32      0.65      0.43      1106\n","     neutral       0.27      0.15      0.19       771\n","         sad       0.25      0.32      0.28       793\n","    surprise       0.40      0.31      0.35       557\n","\n","    accuracy                           0.30      4611\n","   macro avg       0.38      0.23      0.22      4611\n","weighted avg       0.29      0.30      0.26      4611\n","\n"]}],"source":["print(classification_report(y_test_mapped, predicted_classes, target_names=dict_emotions.keys()))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(512, activation='relu')(x)\n","x = Dense(256, activation='relu')(x)\n","predictions = Dense(7, activation='softmax')(x)\n","\n","model = Model(inputs=base_model.input, outputs=predictions)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":2}
